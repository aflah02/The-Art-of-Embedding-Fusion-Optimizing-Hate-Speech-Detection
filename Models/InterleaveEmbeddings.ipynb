{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaling_embeddings(embeddings):\n",
    "#     for i in range(len(embeddings)):\n",
    "#         embeddings[i] = StandardScaler().fit_transform(embeddings[i])\n",
    "#     return embeddings\n",
    "\n",
    "def reshape_embeddings(embeddings)\n",
    "    if embeddings.shape[1] == 1:\n",
    "        embeddings = embeddings.squeeze(1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave_2_embeddings(embeddings1, embeddings2):\n",
    "    embeddings1 = reshape_embeddings(embeddings1)\n",
    "    embeddings2 = reshape_embeddings(embeddings2)\n",
    "    new_embeddings = []\n",
    "    for i in range(len(embeddings1)):\n",
    "        embedding = []\n",
    "        for j in range(len(embeddings1[i])):\n",
    "            embedding.append(embeddings1[i][j])\n",
    "            embedding.append(embeddings2[i][j])\n",
    "        new_embeddings.append(np.array(embedding))\n",
    "    return np.array(new_embeddings)\n",
    "    \n",
    "\n",
    "def interleave_3_embeddings(embeddings1, embeddings2, embeddings3):\n",
    "    embeddings1 = reshape_embeddings(embeddings1)\n",
    "    embeddings2 = reshape_embeddings(embeddings2)\n",
    "    embeddings3 = reshape_embeddings(embeddings3)\n",
    "    new_embeddings = []\n",
    "    for i in range(len(embeddings1)):\n",
    "        embedding = []\n",
    "        for j in range(len(embeddings1[i])):\n",
    "            embedding.append(embeddings1[i][j])\n",
    "            embedding.append(embeddings2[i][j])\n",
    "            embedding.append(embeddings3[i][j])\n",
    "        new_embeddings.append(np.array(embedding))\n",
    "    return np.array(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hatebert_interleaved_dynahate_train = interleave_2_embeddings(bert_dynahate_train_embeddings, hatebert_dynahate_train_embeddings)\n",
    "bert_hatebert_interleaved_dynahate_dev = interleave_2_embeddings(bert_dynahate_dev_embeddings, hatebert_dynahate_dev_embeddings)\n",
    "bert_hatebert_interleaved_dynahate_test = interleave_2_embeddings(bert_dynahate_test_embeddings, hatebert_dynahate_test_embeddings)\n",
    "\n",
    "bert_bertweet_interleaved_dynahate_train = interleave_2_embeddings(bert_dynahate_train_embeddings, bertweet_dynahate_train_embeddings)\n",
    "bert_bertweet_interleaved_dynahate_dev = interleave_2_embeddings(bert_dynahate_dev_embeddings, bertweet_dynahate_dev_embeddings)\n",
    "bert_bertweet_interleaved_dynahate_test = interleave_2_embeddings(bert_dynahate_test_embeddings, bertweet_dynahate_test_embeddings)\n",
    "\n",
    "hatebert_bertweet_interleaved_dynahate_train = interleave_2_embeddings(hatebert_dynahate_train_embeddings, bertweet_dynahate_train_embeddings)\n",
    "hatebert_bertweet_interleaved_dynahate_dev = interleave_2_embeddings(hatebert_dynahate_dev_embeddings, bertweet_dynahate_dev_embeddings)\n",
    "hatebert_bertweet_interleaved_dynahate_test = interleave_2_embeddings(hatebert_dynahate_test_embeddings, bertweet_dynahate_test_embeddings)\n",
    "\n",
    "bert_hatebert_bertweet_interleaved_dynahate_train = interleave_3_embeddings(bert_dynahate_train_embeddings, hatebert_dynahate_train_embeddings, bertweet_dynahate_train_embeddings)\n",
    "bert_hatebert_bertweet_interleaved_dynahate_dev = interleave_3_embeddings(bert_dynahate_dev_embeddings, hatebert_dynahate_dev_embeddings, bertweet_dynahate_dev_embeddings)\n",
    "bert_hatebert_bertweet_interleaved_dynahate_test = interleave_3_embeddings(bert_dynahate_test_embeddings, hatebert_dynahate_test_embeddings, bertweet_dynahate_test_embeddings)\n",
    "\n",
    "bert_hatebert_interleaved_olid_train = interleave_2_embeddings(bert_olid_train_embeddings, hatebert_olid_train_embeddings)\n",
    "bert_hatebert_interleaved_olid_dev = interleave_2_embeddings(bert_olid_dev_embeddings, hatebert_olid_dev_embeddings)\n",
    "bert_hatebert_interleaved_olid_test = interleave_2_embeddings(bert_olid_test_embeddings, hatebert_olid_test_embeddings)\n",
    "\n",
    "bert_bertweet_interleaved_olid_train = interleave_2_embeddings(bert_olid_train_embeddings, bertweet_olid_train_embeddings)\n",
    "bert_bertweet_interleaved_olid_dev = interleave_2_embeddings(bert_olid_dev_embeddings, bertweet_olid_dev_embeddings)\n",
    "bert_bertweet_interleaved_olid_test = interleave_2_embeddings(bert_olid_test_embeddings, bertweet_olid_test_embeddings)\n",
    "\n",
    "hatebert_bertweet_interleaved_olid_train = interleave_2_embeddings(hatebert_olid_train_embeddings, bertweet_olid_train_embeddings)\n",
    "hatebert_bertweet_interleaved_olid_dev = interleave_2_embeddings(hatebert_olid_dev_embeddings, bertweet_olid_dev_embeddings)\n",
    "hatebert_bertweet_interleaved_olid_test = interleave_2_embeddings(hatebert_olid_test_embeddings, bertweet_olid_test_embeddings)\n",
    "\n",
    "bert_hatebert_bertweet_interleaved_olid_train = interleave_3_embeddings(bert_olid_train_embeddings, hatebert_olid_train_embeddings, bertweet_olid_train_embeddings)\n",
    "bert_hatebert_bertweet_interleaved_olid_dev = interleave_3_embeddings(bert_olid_dev_embeddings, hatebert_olid_dev_embeddings, bertweet_olid_dev_embeddings)\n",
    "bert_hatebert_bertweet_interleaved_olid_test = interleave_3_embeddings(bert_olid_test_embeddings, hatebert_olid_test_embeddings, bertweet_olid_test_embeddings)\n",
    "\n",
    "bert_hatebert_interleaved_latenthatred_train = interleave_2_embeddings(bert_latenthatred_train_embeddings, hatebert_latenthatred_train_embeddings)\n",
    "bert_hatebert_interleaved_latenthatred_dev = interleave_2_embeddings(bert_latenthatred_dev_embeddings, hatebert_latenthatred_dev_embeddings)\n",
    "bert_hatebert_interleaved_latenthatred_test = interleave_2_embeddings(bert_latenthatred_test_embeddings, hatebert_latenthatred_test_embeddings)\n",
    "\n",
    "bert_bertweet_interleaved_latenthatred_train = interleave_2_embeddings(bert_latenthatred_train_embeddings, bertweet_latenthatred_train_embeddings)\n",
    "bert_bertweet_interleaved_latenthatred_dev = interleave_2_embeddings(bert_latenthatred_dev_embeddings, bertweet_latenthatred_dev_embeddings)\n",
    "bert_bertweet_interleaved_latenthatred_test = interleave_2_embeddings(bert_latenthatred_test_embeddings, bertweet_latenthatred_test_embeddings)\n",
    "\n",
    "hatebert_bertweet_interleaved_latenthatred_train = interleave_2_embeddings(hatebert_latenthatred_train_embeddings, bertweet_latenthatred_train_embeddings)\n",
    "hatebert_bertweet_interleaved_latenthatred_dev = interleave_2_embeddings(hatebert_latenthatred_dev_embeddings, bertweet_latenthatred_dev_embeddings)\n",
    "hatebert_bertweet_interleaved_latenthatred_test = interleave_2_embeddings(hatebert_latenthatred_test_embeddings, bertweet_latenthatred_test_embeddings)\n",
    "\n",
    "bert_hatebert_bertweet_interleaved_latenthatred_train = interleave_3_embeddings(bert_latenthatred_train_embeddings, hatebert_latenthatred_train_embeddings, bertweet_latenthatred_train_embeddings)\n",
    "bert_hatebert_bertweet_interleaved_latenthatred_dev = interleave_3_embeddings(bert_latenthatred_dev_embeddings, hatebert_latenthatred_dev_embeddings, bertweet_latenthatred_dev_embeddings)\n",
    "bert_hatebert_bertweet_interleaved_latenthatred_test = interleave_3_embeddings(bert_latenthatred_test_embeddings, hatebert_latenthatred_test_embeddings, bertweet_latenthatred_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5370, 2304)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_hatebert_bertweet_interleaved_latenthatred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynahate_labels_train = process_labels(read_labels(\"dynahate\", \"train\"))\n",
    "dynahate_labels_dev = process_labels(read_labels(\"dynahate\", \"dev\"))\n",
    "dynahate_labels_test = process_labels(read_labels(\"dynahate\", \"test\"))\n",
    "\n",
    "latenthatred_labels_train = read_labels(\"latenthatred\", \"train\")\n",
    "latenthatred_labels_dev = read_labels(\"latenthatred\", \"dev\")\n",
    "latenthatred_labels_test = read_labels(\"latenthatred\", \"test\")\n",
    "\n",
    "olid_labels_train = read_labels(\"olid\", \"train\")\n",
    "olid_labels_dev = read_labels(\"olid\", \"dev\")\n",
    "olid_labels_test = read_labels(\"olid\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "gridsearch = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid={\n",
    "        \"hidden_layer_sizes\": [(128), (128,64)],\n",
    "        \"activation\": [\"relu\"],\n",
    "        \"solver\": [\"adam\"],\n",
    "        \"learning_rate_init\": [0.001, 0.0001],\n",
    "        \"learning_rate\": [\"adaptive\"],\n",
    "        \"early_stopping\": [True],\n",
    "        \"max_iter\": [10000]\n",
    "    },\n",
    "    verbose=4,\n",
    "    n_jobs=os.cpu_count()//3,\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynahate_labels_train_dev = np.concatenate((dynahate_labels_train, dynahate_labels_dev))\n",
    "latenthatred_labels_train_dev = np.concatenate((latenthatred_labels_train, latenthatred_labels_dev))\n",
    "olid_labels_train_dev = np.concatenate((olid_labels_train, olid_labels_dev))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-BERTweet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DynaHate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params:  {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (128, 64), 'learning_rate': 'adaptive', 'learning_rate_init': 0.0001, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Accuracy Train Dev:  0.7403305963699223\n",
      "Accuracy Test:  0.675242718446602\n",
      "Weighted F1 Train Dev:  0.7395148064067016\n",
      "Weighted F1 Test:  0.6748111324754928\n",
      "Macro F1 Train Dev:  0.7374381148032481\n",
      "Macro F1 Test:  0.6710707207258813\n",
      "Micro F1 Train Dev:  0.7403305963699223\n",
      "Micro F1 Test:  0.675242718446602\n",
      "Weighted Recall Train Dev:  0.7403305963699223\n",
      "Weighted Recall Test:  0.675242718446602\n",
      "Macro Recall Train Dev:  0.7366041395521248\n",
      "Macro Recall Test:  0.6707092004068246\n",
      "Micro Recall Train Dev:  0.7403305963699223\n",
      "Micro Recall Test:  0.675242718446602\n",
      "Confusion Matrix Train Dev: \n",
      "[[11762  5355]\n",
      " [ 4259 15648]]\n",
      "Confusion Matrix Test: \n",
      "[[1159  693]\n",
      " [ 645 1623]]\n"
     ]
    }
   ],
   "source": [
    "bert_bertweet_interleaved_dynahate_train_dev = np.concatenate((bert_bertweet_interleaved_dynahate_train, bert_bertweet_interleaved_dynahate_dev))\n",
    "bert_bertweet_interleaved_dynahate_labels_train_dev = np.concatenate((dynahate_labels_train, dynahate_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_bertweet_interleaved_dynahate_train_dev, bert_bertweet_interleaved_dynahate_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_bertweet_interleaved_dynahate_train_dev)\n",
    "test_preds = mlp.predict(bert_bertweet_interleaved_dynahate_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, dynahate_labels_train_dev, dynahate_labels_test, \"Results/bert_bertweet_interleaved_dynahate\")\n",
    "save_model(mlp, \"Saves/bert_bertweet_interleaved_dynahate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LatentHatred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params:  {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (128, 64), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Accuracy Train Dev:  0.6980757293606455\n",
      "Accuracy Test:  0.6919925512104284\n",
      "Weighted F1 Train Dev:  0.6694827554060424\n",
      "Weighted F1 Test:  0.6610677436972126\n",
      "Macro F1 Train Dev:  0.48666441471128535\n",
      "Macro F1 Test:  0.47300468470146856\n",
      "Micro F1 Train Dev:  0.6980757293606455\n",
      "Micro F1 Test:  0.6919925512104284\n",
      "Weighted Recall Train Dev:  0.6980757293606455\n",
      "Weighted Recall Test:  0.6919925512104284\n",
      "Macro Recall Train Dev:  0.47061806770853254\n",
      "Macro Recall Test:  0.45938134394445895\n",
      "Micro Recall Train Dev:  0.6980757293606455\n",
      "Micro Recall Test:  0.6919925512104284\n",
      "Confusion Matrix Train Dev: \n",
      "[[8882 1036   27]\n",
      " [3006 2290   51]\n",
      " [ 424  320   74]]\n",
      "Confusion Matrix Test: \n",
      "[[2995  335   16]\n",
      " [1037  698   18]\n",
      " [ 134  114   23]]\n"
     ]
    }
   ],
   "source": [
    "bert_bertweet_interleaved_latenthatred_train_dev = np.concatenate((bert_bertweet_interleaved_latenthatred_train, bert_bertweet_interleaved_latenthatred_dev))\n",
    "bert_bertweet_interleaved_latenthatred_labels_train_dev = np.concatenate((latenthatred_labels_train, latenthatred_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_bertweet_interleaved_latenthatred_train_dev, bert_bertweet_interleaved_latenthatred_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_bertweet_interleaved_latenthatred_train_dev)\n",
    "test_preds = mlp.predict(bert_bertweet_interleaved_latenthatred_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, latenthatred_labels_train_dev, latenthatred_labels_test, \"Results/bert_bertweet_interleaved_latenthatred\")\n",
    "save_model(mlp, \"Saves/bert_bertweet_interleaved_latenthatred\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params:  {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': 128, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Accuracy Train Dev:  0.7694864048338369\n",
      "Accuracy Test:  0.8046511627906977\n",
      "Weighted F1 Train Dev:  0.7543551849579516\n",
      "Weighted F1 Test:  0.7851929737753588\n",
      "Macro F1 Train Dev:  0.7105594768426122\n",
      "Macro F1 Test:  0.7140957446808511\n",
      "Micro F1 Train Dev:  0.769486404833837\n",
      "Micro F1 Test:  0.8046511627906977\n",
      "Weighted Recall Train Dev:  0.7694864048338369\n",
      "Weighted Recall Test:  0.8046511627906977\n",
      "Macro Recall Train Dev:  0.6965019539284245\n",
      "Macro Recall Test:  0.6908602150537635\n",
      "Micro Recall Train Dev:  0.7694864048338369\n",
      "Micro Recall Test:  0.8046511627906977\n",
      "Confusion Matrix Train Dev: \n",
      "[[8081  759]\n",
      " [2293 2107]]\n",
      "Confusion Matrix Test: \n",
      "[[588  32]\n",
      " [136 104]]\n"
     ]
    }
   ],
   "source": [
    "bert_bertweet_interleaved_olid_train_dev = np.concatenate((bert_bertweet_interleaved_olid_train, bert_bertweet_interleaved_olid_dev))\n",
    "bert_bertweet_interleaved_olid_labels_train_dev = np.concatenate((olid_labels_train, olid_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_bertweet_interleaved_olid_train_dev, bert_bertweet_interleaved_olid_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_bertweet_interleaved_olid_train_dev)\n",
    "test_preds = mlp.predict(bert_bertweet_interleaved_olid_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, olid_labels_train_dev, olid_labels_test, \"Results/bert_bertweet_interleaved_olid\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_bertweet_interleaved_olid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-HateBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DynaHate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "13 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 428, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 638, in _fit_stochastic\n",
      "    X_batch = _safe_indexing(X, sample_idx[batch_slice])\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 361, in _safe_indexing\n",
      "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 185, in _array_indexing\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.17 MiB for an array with shape (200, 1536) and data type float32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 428, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 659, in _fit_stochastic\n",
      "    self._optimizer.update_params(params, grads)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\", line 42, in update_params\n",
      "    updates = self._get_updates(grads)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\", line 284, in _get_updates\n",
      "    updates = [\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\", line 285, in <listcomp>\n",
      "    -self.learning_rate * m / (np.sqrt(v) + self.epsilon)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 768. KiB for an array with shape (1536, 128) and data type float32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 428, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 659, in _fit_stochastic\n",
      "    self._optimizer.update_params(params, grads)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\", line 42, in update_params\n",
      "    updates = self._get_updates(grads)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\", line 271, in _get_updates\n",
      "    self.ms = [\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_stochastic_optimizers.py\", line 272, in <listcomp>\n",
      "    self.beta_1 * m + (1 - self.beta_1) * grad\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 768. KiB for an array with shape (1536, 128) and data type float32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 428, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 601, in _fit_stochastic\n",
      "    X, X_val, y, y_val = train_test_split(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py\", line 2456, in train_test_split\n",
      "    return list(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py\", line 2458, in <genexpr>\n",
      "    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 361, in _safe_indexing\n",
      "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 185, in _array_indexing\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 17.4 MiB for an array with shape (2962, 1536) and data type float32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 428, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 601, in _fit_stochastic\n",
      "    X, X_val, y, y_val = train_test_split(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py\", line 2456, in train_test_split\n",
      "    return list(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py\", line 2458, in <genexpr>\n",
      "    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 361, in _safe_indexing\n",
      "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 185, in _array_indexing\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 156. MiB for an array with shape (26657, 1536) and data type float32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 428, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 601, in _fit_stochastic\n",
      "    X, X_val, y, y_val = train_test_split(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py\", line 2456, in train_test_split\n",
      "    return list(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_split.py\", line 2458, in <genexpr>\n",
      "    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 361, in _safe_indexing\n",
      "    return _array_indexing(X, indices, indices_dtype, axis=axis)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\__init__.py\", line 185, in _array_indexing\n",
      "    return array[key] if axis == 0 else array[:, key]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 156. MiB for an array with shape (26658, 1536) and data type float32\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': 128, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Accuracy Train Dev:  0.7331190579083837\n",
      "Accuracy Test:  0.6927184466019417\n",
      "Weighted F1 Train Dev:  0.7307496652997779\n",
      "Weighted F1 Test:  0.6907637873891334\n",
      "Macro F1 Train Dev:  0.727913712756659\n",
      "Macro F1 Test:  0.6861955950715588\n",
      "Micro F1 Train Dev:  0.7331190579083836\n",
      "Micro F1 Test:  0.6927184466019417\n",
      "Weighted Recall Train Dev:  0.7331190579083837\n",
      "Weighted Recall Test:  0.6927184466019417\n",
      "Macro Recall Train Dev:  0.72682341204648\n",
      "Macro Recall Test:  0.6851461406896973\n",
      "Micro Recall Train Dev:  0.7331190579083837\n",
      "Micro Recall Test:  0.6927184466019417\n",
      "Confusion Matrix Train Dev: \n",
      "[[11011  6106]\n",
      " [ 3775 16132]]\n",
      "Confusion Matrix Test: \n",
      "[[1130  722]\n",
      " [ 544 1724]]\n"
     ]
    }
   ],
   "source": [
    "bert_hatebert_interleaved_dynahate_train_dev = np.concatenate((bert_hatebert_interleaved_dynahate_train, bert_hatebert_interleaved_dynahate_dev))\n",
    "bert_hatebert_interleaved_dynahate_labels_train_dev = np.concatenate((dynahate_labels_train, dynahate_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_hatebert_interleaved_dynahate_train_dev, bert_hatebert_interleaved_dynahate_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_hatebert_interleaved_dynahate_train_dev)\n",
    "test_preds = mlp.predict(bert_hatebert_interleaved_dynahate_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, dynahate_labels_train_dev, dynahate_labels_test, \"Results/bert_hatebert_interleaved_dynahate\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_hatebert_interleaved_dynahate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LatentHatred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params:  {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': 128, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Accuracy Train Dev:  0.7015518311607697\n",
      "Accuracy Test:  0.6893854748603352\n",
      "Weighted F1 Train Dev:  0.6741768004536677\n",
      "Weighted F1 Test:  0.6602346636537656\n",
      "Macro F1 Train Dev:  0.4879189214138211\n",
      "Macro F1 Test:  0.4742221825273883\n",
      "Micro F1 Train Dev:  0.7015518311607697\n",
      "Micro F1 Test:  0.6893854748603352\n",
      "Weighted Recall Train Dev:  0.7015518311607697\n",
      "Weighted Recall Test:  0.6893854748603352\n",
      "Macro Recall Train Dev:  0.4729886073581921\n",
      "Macro Recall Test:  0.4597972194077726\n",
      "Micro Recall Train Dev:  0.7015518311607697\n",
      "Micro Recall Test:  0.6893854748603352\n",
      "Confusion Matrix Train Dev: \n",
      "[[8861 1053   31]\n",
      " [2931 2372   44]\n",
      " [ 382  367   69]]\n",
      "Confusion Matrix Test: \n",
      "[[2961  375   10]\n",
      " [1018  718   17]\n",
      " [ 139  109   23]]\n"
     ]
    }
   ],
   "source": [
    "bert_hatebert_interleaved_latenthatred_train_dev = np.concatenate((bert_hatebert_interleaved_latenthatred_train, bert_hatebert_interleaved_latenthatred_dev))\n",
    "bert_hatebert_interleaved_latenthatred_labels_train_dev = np.concatenate((latenthatred_labels_train, latenthatred_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_hatebert_interleaved_latenthatred_train_dev, bert_hatebert_interleaved_latenthatred_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_hatebert_interleaved_latenthatred_train_dev)\n",
    "test_preds = mlp.predict(bert_hatebert_interleaved_latenthatred_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, latenthatred_labels_train_dev, latenthatred_labels_test, \"Results/bert_hatebert_interleaved_latenthatred\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_hatebert_interleaved_latenthatred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params:  {'activation': 'relu', 'early_stopping': True, 'hidden_layer_sizes': (128, 64), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 10000, 'solver': 'adam'}\n",
      "Accuracy Train Dev:  0.763368580060423\n",
      "Accuracy Test:  0.7872093023255814\n",
      "Weighted F1 Train Dev:  0.750721977034511\n",
      "Weighted F1 Test:  0.7594893151394954\n",
      "Macro F1 Train Dev:  0.7081511525955055\n",
      "Macro F1 Test:  0.6752205540937937\n",
      "Micro F1 Train Dev:  0.7633685800604229\n",
      "Micro F1 Test:  0.7872093023255814\n",
      "Weighted Recall Train Dev:  0.763368580060423\n",
      "Weighted Recall Test:  0.7872093023255814\n",
      "Macro Recall Train Dev:  0.695744549568079\n",
      "Macro Recall Test:  0.6557795698924731\n",
      "Micro Recall Train Dev:  0.763368580060423\n",
      "Micro Recall Test:  0.7872093023255814\n",
      "Confusion Matrix Train Dev: \n",
      "[[7933  907]\n",
      " [2226 2174]]\n",
      "Confusion Matrix Test: \n",
      "[[591  29]\n",
      " [154  86]]\n"
     ]
    }
   ],
   "source": [
    "bert_hatebert_interleaved_olid_train_dev = np.concatenate((bert_hatebert_interleaved_olid_train, bert_hatebert_interleaved_olid_dev))\n",
    "bert_hatebert_interleaved_olid_labels_train_dev = np.concatenate((olid_labels_train, olid_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_hatebert_interleaved_olid_train_dev, bert_hatebert_interleaved_olid_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_hatebert_interleaved_olid_train_dev)\n",
    "test_preds = mlp.predict(bert_hatebert_interleaved_olid_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, olid_labels_train_dev, olid_labels_test, \"Results/bert_hatebert_interleaved_olid\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_hatebert_interleaved_olid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTweet-HateBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DynaHate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "hatebert_bertweet_interleaved_dynahate_train_dev = np.concatenate((hatebert_bertweet_interleaved_dynahate_train, hatebert_bertweet_interleaved_dynahate_dev))\n",
    "hatebert_bertweet_interleaved_dynahate_labels_train_dev = np.concatenate((dynahate_labels_train, dynahate_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(hatebert_bertweet_interleaved_dynahate_train_dev, hatebert_bertweet_interleaved_dynahate_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(hatebert_bertweet_interleaved_dynahate_train_dev)\n",
    "test_preds = mlp.predict(hatebert_bertweet_interleaved_dynahate_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, dynahate_labels_train_dev, dynahate_labels_test, \"Results/hatebert_bertweet_interleaved_dynahate\")\n",
    "\n",
    "save_model(mlp, \"Saves/hatebert_bertweet_interleaved_dynahate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LatentHatred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebert_bertweet_interleaved_latenthatred_train_dev = np.concatenate((hatebert_bertweet_interleaved_latenthatred_train, hatebert_bertweet_interleaved_latenthatred_dev))\n",
    "hatebert_bertweet_interleaved_latenthatred_labels_train_dev = np.concatenate((latenthatred_labels_train, latenthatred_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(hatebert_bertweet_interleaved_latenthatred_train_dev, hatebert_bertweet_interleaved_latenthatred_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(hatebert_bertweet_interleaved_latenthatred_train_dev)\n",
    "test_preds = mlp.predict(hatebert_bertweet_interleaved_latenthatred_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, latenthatred_labels_train_dev, latenthatred_labels_test, \"Results/hatebert_bertweet_interleaved_latenthatred\")\n",
    "\n",
    "save_model(mlp, \"Saves/hatebert_bertweet_interleaved_latenthatred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebert_bertweet_interleaved_olid_train_dev = np.concatenate((hatebert_bertweet_interleaved_olid_train, hatebert_bertweet_interleaved_olid_dev))\n",
    "hatebert_bertweet_interleaved_olid_labels_train_dev = np.concatenate((olid_labels_train, olid_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(hatebert_bertweet_interleaved_olid_train_dev, hatebert_bertweet_interleaved_olid_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(hatebert_bertweet_interleaved_olid_train_dev)\n",
    "test_preds = mlp.predict(hatebert_bertweet_interleaved_olid_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, olid_labels_train_dev, olid_labels_test, \"Results/hatebert_bertweet_interleaved_olid\")\n",
    "\n",
    "save_model(mlp, \"Saves/hatebert_bertweet_interleaved_olid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT-BERTweet-HateBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DynaHate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_bertweet_hatebert_interleaved_dynahate_train_dev = np.concatenate((bert_hatebert_bertweet_interleaved_dynahate_train, bert_hatebert_bertweet_interleaved_dynahate_dev))\n",
    "bert_bertweet_hatebert_interleaved_dynahate_labels_train_dev = np.concatenate((dynahate_labels_train, dynahate_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_bertweet_hatebert_interleaved_dynahate_train_dev, bert_bertweet_hatebert_interleaved_dynahate_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_bertweet_hatebert_interleaved_dynahate_train_dev)\n",
    "test_preds = mlp.predict(bert_hatebert_bertweet_interleaved_dynahate_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, dynahate_labels_train_dev, dynahate_labels_test, \"Results/bert_bertweet_hatebert_interleaved_dynahate\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_bertweet_hatebert_interleaved_dynahate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LatentHatred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_bertweet_hatebert_interleaved_latenthatred_train_dev = np.concatenate((bert_hatebert_bertweet_interleaved_latenthatred_train, bert_hatebert_bertweet_interleaved_latenthatred_dev))\n",
    "bert_bertweet_hatebert_interleaved_latenthatred_labels_train_dev = np.concatenate((latenthatred_labels_train, latenthatred_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_bertweet_hatebert_interleaved_latenthatred_train_dev, bert_bertweet_hatebert_interleaved_latenthatred_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_bertweet_hatebert_interleaved_latenthatred_train_dev)\n",
    "test_preds = mlp.predict(bert_hatebert_bertweet_interleaved_latenthatred_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, latenthatred_labels_train_dev, latenthatred_labels_test, \"Results/bert_bertweet_hatebert_interleaved_latenthatred\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_bertweet_hatebert_interleaved_latenthatred\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_bertweet_hatebert_interleaved_olid_train_dev = np.concatenate((bert_hatebert_bertweet_interleaved_olid_train, bert_hatebert_bertweet_interleaved_olid_dev))\n",
    "bert_bertweet_hatebert_interleaved_olid_labels_train_dev = np.concatenate((olid_labels_train, olid_labels_dev))\n",
    "\n",
    "grid_results = gridsearch.fit(bert_bertweet_hatebert_interleaved_olid_train_dev, bert_bertweet_hatebert_interleaved_olid_labels_train_dev)\n",
    "\n",
    "best_params = grid_results.best_params_\n",
    "mlp = grid_results.best_estimator_\n",
    "\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "train_dev_preds = mlp.predict(bert_bertweet_hatebert_interleaved_olid_train_dev)\n",
    "test_preds = mlp.predict(bert_hatebert_bertweet_interleaved_olid_test)\n",
    "\n",
    "computeAllScores(train_dev_preds, test_preds, olid_labels_train_dev, olid_labels_test, \"Results/bert_bertweet_hatebert_interleaved_olid\")\n",
    "\n",
    "save_model(mlp, \"Saves/bert_bertweet_hatebert_interleaved_olid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
